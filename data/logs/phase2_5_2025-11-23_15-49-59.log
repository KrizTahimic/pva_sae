2025-11-23 15:49:59,760 [INFO] [sae_analyzer] setup_logging:167 - Phase: 2.5, GPU: CPU
2025-11-23 15:49:59,761 [INFO] [phase5_3.weight_orthogonalizer] setup_logging:167 - Phase: 5.3, GPU: CPU
2025-11-23 15:49:59,761 [INFO] [main] run_phase5_3:1026 - Starting Phase 5.3: Weight Orthogonalization Analysis
2025-11-23 15:49:59,761 [INFO] [main] run_phase5_3:1027 - Will auto-discover PVA features from Phase 2.5 and baseline from Phase 3.5
2025-11-23 15:49:59,761 [INFO] [main] run_phase5_3:1028 - This phase permanently modifies model weights to remove PVA information
2025-11-23 15:49:59,762 [INFO] [main] run_phase5_3:1031 - 
============================================================
CONFIGURATION
============================================================

ACTIVATION Settings:
  activation_cleanup_after_batch: True
  activation_clear_cache_between_layers: True
  activation_hook_type: resid_post
  activation_layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
  activation_max_cache_gb: 10.0
  activation_max_length: 2048
  activation_position: -1

AUTOSAVE Settings:
  autosave_frequency: 100
  autosave_keep_last: 3

CHECKPOINT Settings:
  checkpoint_dir: checkpoints
  checkpoint_frequency: 50

CONTINUE Settings:
  continue_on_error: True

DATASET Settings:
  dataset_dir: data/phase1_0
  dataset_end_idx: None
  dataset_name: humaneval
  dataset_split: test
  dataset_start_idx: 0

ENABLE Settings:
  enable_timing_stats: True

EVALUATION Settings:
  evaluation_random_seed: 42

GC Settings:
  gc_collect_frequency: 50

LOG Settings:
  log_dir: data/logs

MAX Settings:
  max_gpu_memory_usage_gb: 30.0
  max_memory_usage_gb: 100.0
  max_retries: 3

MEMORY Settings:
  memory_cleanup_frequency: 100

MODEL Settings:
  model_device: None
  model_dtype: None
  model_max_new_tokens: 800
  model_name: google/gemma-2-2b
  model_temperature: 0.0
  model_trust_remote_code: True

ORTHOGONALIZATION Settings:
  orthogonalization_target_weights: [embed, attn_o, mlp_down]

PHASE0 Settings:
  phase0_1_output_dir: data/phase0_1
  phase0_2_output_dir: data/phase0_2_humaneval
  phase0_3_output_dir: data/phase0_3_humaneval
  phase0_output_dir: data/phase0

PHASE1 Settings:
  phase1_output_dir: data/phase1_0

PHASE2 Settings:
  phase2_10_output_dir: data/phase2_10
  phase2_15_output_dir: data/phase2_15
  phase2_2_output_dir: data/phase2_2
  phase2_5_output_dir: data/phase2_5
  phase2_output_dir: data/phase2

PHASE3 Settings:
  phase3_10_output_dir: data/phase3_10
  phase3_10_temperatures: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4]
  phase3_11_output_dir: data/phase3_11
  phase3_12_output_dir: data/phase3_12
  phase3_5_output_dir: data/phase3_5
  phase3_6_output_dir: data/phase3_6
  phase3_8_output_dir: data/phase3_8
  phase3_output_dir: data/phase3

PHASE4 Settings:
  phase4_10_min_activation_freq: 0.001
  phase4_10_n_features: 10
  phase4_10_output_dir: data/phase4_10
  phase4_10_separation_threshold: 0.01
  phase4_12_output_dir: data/phase4_12
  phase4_14_output_dir: data/phase4_14
  phase4_14_significance_level: 0.05
  phase4_5_correct_coefficients: [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
  phase4_5_experiment_mode: all
  phase4_5_incorrect_coefficients: [100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0]
  phase4_5_meaningful_effect_threshold: 5.0
  phase4_5_output_dir: data/phase4_5
  phase4_5_plateau_threshold: 2.0
  phase4_5_search_tolerance: 2.0
  phase4_6_experiment_mode: all
  phase4_6_output_dir: data/phase4_6
  phase4_6_tolerance: 1.0
  phase4_8_correct_coefficient: 29
  phase4_8_experiment_mode: all
  phase4_8_incorrect_coefficient: 287
  phase4_8_output_dir: data/phase4_8

PHASE5 Settings:
  **phase5_3_output_dir**: data/phase5_3
  phase5_6_output_dir: data/phase5_6
  phase5_9_output_dir: data/phase5_9
  phase5_9_significance_level: 0.05

PHASE6 Settings:
  phase6_3_output_dir: data/phase6_3

PHASE7 Settings:
  phase7_12_output_dir: data/phase7_12
  phase7_3_model_name: google/gemma-2-2b-it
  phase7_3_output_dir: data/phase7_3
  phase7_6_model_name: google/gemma-2-2b-it
  phase7_6_output_dir: data/phase7_6

PHASE8 Settings:
  phase8_1_output_dir: data/phase8_1
  phase8_2_output_dir: data/phase8_2
  phase8_3_output_dir: data/phase8_3
  phase8_3_percentile: 70.0
  phase8_3_use_percentile_threshold: True

PILE Settings:
  pile_filter_enabled: True
  pile_samples: 10000
  pile_threshold: 0.02

PROGRESS Settings:
  progress_log_frequency: 10

RETRY Settings:
  retry_backoff: 1.0

SAE Settings:
  sae_checkpoint_dir: data/phase2/sae_checkpoints
  sae_cleanup_after_layer: True
  sae_hook_component: resid_post
  sae_latent_threshold: 0.02
  sae_repo_id: google/gemma-scope-2b-pt-res
  sae_save_after_each_layer: True
  sae_sparsity: 71
  sae_use_memory_mapping: False
  sae_width: 16k

SHOW Settings:
  show_progress_bar: True

SPLIT Settings:
  split_n_strata: 10
  split_random_seed: 42
  split_ratio_tolerance: 0.02

T Settings:
  t_statistic_min_samples: 10

TEMPERATURE Settings:
  temperature_samples_per_temp: 3
  temperature_variation_temps: [0.0]

TIMEOUT Settings:
  timeout_per_record: 300.0

VERBOSE Settings:
  verbose: False

============================================================
2025-11-23 15:49:59,763 [INFO] [phase5_3.weight_orthogonalizer] __init__:63 - Output directory: data/phase5_3_humaneval
2025-11-23 15:49:59,763 [INFO] [phase5_3.weight_orthogonalizer] _load_dependencies:83 - Loading PVA features from Phase 2.5...
2025-11-23 15:49:59,764 [INFO] [phase5_3.weight_orthogonalizer] _load_dependencies:106 - Best correct feature: Layer 16, Index 11225, Score 0.2212
2025-11-23 15:49:59,764 [INFO] [phase5_3.weight_orthogonalizer] _load_dependencies:109 - Best incorrect feature: Layer 25, Index 2853, Score 0.2010
2025-11-23 15:49:59,764 [INFO] [phase5_3.weight_orthogonalizer] _load_dependencies:114 - Loading baseline data from Phase 3.5...
2025-11-23 15:49:59,788 [INFO] [phase5_3.weight_orthogonalizer] _load_dependencies:126 - Loaded 164 problems from Phase 3.5 baseline
2025-11-23 15:49:59,788 [INFO] [phase5_3.weight_orthogonalizer] _load_dependencies:147 - Loading SAE models...
2025-11-23 15:49:59,788 [INFO] [sae_analyzer] load_gemma_scope_sae:47 - Loading GemmaScope SAE for layer 16
2025-11-23 15:49:59,788 [INFO] [sae_analyzer] load_gemma_scope_sae:61 - Loading from HuggingFace: google/gemma-scope-2b-pt-res/layer_16/width_16k/average_l0_78/params.npz
2025-11-23 15:49:59,788 [INFO] [sae_analyzer] load_gemma_scope_sae:62 - This may take a while if the model is not cached...
2025-11-23 15:50:00,413 [INFO] [sae_analyzer] load_gemma_scope_sae:70 - Download complete, loading from: /home/kriz_tahimic/.cache/huggingface/hub/models--google--gemma-scope-2b-pt-res/snapshots/fd571b47c1c64851e9b1989792367b9babb4af63/layer_16/width_16k/average_l0_78/params.npz
2025-11-23 15:50:01,565 [INFO] [sae_analyzer] load_gemma_scope_sae:87 - Loaded SAE with d_model=2304, d_sae=16384, sparsity=78
2025-11-23 15:50:01,566 [INFO] [sae_analyzer] load_gemma_scope_sae:47 - Loading GemmaScope SAE for layer 25
2025-11-23 15:50:01,566 [INFO] [sae_analyzer] load_gemma_scope_sae:61 - Loading from HuggingFace: google/gemma-scope-2b-pt-res/layer_25/width_16k/average_l0_116/params.npz
2025-11-23 15:50:01,566 [INFO] [sae_analyzer] load_gemma_scope_sae:62 - This may take a while if the model is not cached...
2025-11-23 15:50:01,769 [INFO] [sae_analyzer] load_gemma_scope_sae:70 - Download complete, loading from: /home/kriz_tahimic/.cache/huggingface/hub/models--google--gemma-scope-2b-pt-res/snapshots/fd571b47c1c64851e9b1989792367b9babb4af63/layer_25/width_16k/average_l0_116/params.npz
2025-11-23 15:50:02,775 [INFO] [sae_analyzer] load_gemma_scope_sae:87 - Loaded SAE with d_model=2304, d_sae=16384, sparsity=116
2025-11-23 15:50:02,776 [INFO] [phase5_3.weight_orthogonalizer] _load_dependencies:161 - SAE decoder directions extracted successfully
2025-11-23 15:50:02,778 [INFO] [phase5_3.weight_orthogonalizer] _split_baseline_by_correctness:168 - Baseline split: 25 correct, 139 incorrect
2025-11-23 15:50:02,778 [INFO] [phase5_3.weight_orthogonalizer] __init__:78 - WeightOrthogonalizer initialized successfully
2025-11-23 15:50:02,778 [INFO] [phase5_3.weight_orthogonalizer] run:780 - 
============================================================
2025-11-23 15:50:02,778 [INFO] [phase5_3.weight_orthogonalizer] run:781 - Starting Phase 5.3: Weight Orthogonalization Analysis
2025-11-23 15:50:02,779 [INFO] [phase5_3.weight_orthogonalizer] run:782 - ============================================================
2025-11-23 15:50:02,779 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:282 - 
============================================================
2025-11-23 15:50:02,779 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:283 - Applying INCORRECT feature orthogonalization
2025-11-23 15:50:02,779 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:284 - ============================================================
2025-11-23 15:50:02,779 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:287 - Loading fresh model for incorrect orthogonalization...
2025-11-23 15:50:02,780 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:47 - Loading model google/gemma-2-2b on cuda with dtype torch.bfloat16
2025-11-23 15:50:04,241 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:56 - Loading model to cuda...
2025-11-23 15:50:08,687 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:86 - Model successfully loaded on cuda:0
2025-11-23 15:50:08,687 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:88 - Model loaded successfully: Gemma2ForCausalLM
2025-11-23 15:50:08,688 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:89 - Model size: 2.61B parameters
2025-11-23 15:50:08,690 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:296 - Orthogonalizing weights to remove incorrect feature...
2025-11-23 15:50:09,267 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:66 - Orthogonalized embeddings, change magnitude: 13.5625
2025-11-23 15:50:09,312 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:120 - Total weight change magnitude: 46.4902
2025-11-23 15:50:09,312 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:121 - Modified 53 weight matrices
2025-11-23 15:50:09,313 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:129 - Average attention change: 0.5122
2025-11-23 15:50:09,313 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:131 - Average MLP change: 0.7542
2025-11-23 15:50:09,313 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:304 - 
Testing on initially incorrect problems...
2025-11-23 15:59:09,132 [INFO] [phase5_3.weight_orthogonalizer] save_checkpoint:190 - Saved incorrect_ortho/incorrect checkpoint at index 49/138
2025-11-23 16:13:01,786 [INFO] [phase5_3.weight_orthogonalizer] save_checkpoint:190 - Saved incorrect_ortho/incorrect checkpoint at index 99/138
2025-11-23 16:13:01,786 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:377 - Autosaving at task 100/139
2025-11-23 16:13:01,788 [INFO] [phase5_3.weight_orthogonalizer] save_checkpoint:190 - Saved incorrect_ortho/incorrect checkpoint at index 99/138
2025-11-23 16:22:27,768 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:381 - 
Testing on initially correct problems...
2025-11-23 16:25:52,166 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:495 - 
Results for INCORRECT orthogonalization:
2025-11-23 16:25:52,167 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:496 -   Correction rate: 0.7% (1/139)
2025-11-23 16:25:52,167 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:497 -   Preservation rate: 92.0% (23/25)
2025-11-23 16:25:52,167 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:498 -   Correction p-value: 1.0000 (not significant)
2025-11-23 16:25:52,167 [INFO] [phase5_3.weight_orthogonalizer] apply_incorrect_orthogonalization:499 -   Preservation p-value: 0.0000 (significant)
2025-11-23 16:25:52,405 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:515 - 
============================================================
2025-11-23 16:25:52,407 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:516 - Applying CORRECT feature orthogonalization
2025-11-23 16:25:52,408 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:517 - ============================================================
2025-11-23 16:25:52,408 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:520 - Loading fresh model for correct orthogonalization...
2025-11-23 16:25:52,408 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:47 - Loading model google/gemma-2-2b on cuda with dtype torch.bfloat16
2025-11-23 16:25:53,938 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:56 - Loading model to cuda...
2025-11-23 16:25:58,449 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:86 - Model successfully loaded on cuda:0
2025-11-23 16:25:58,449 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:88 - Model loaded successfully: Gemma2ForCausalLM
2025-11-23 16:25:58,450 [INFO] [common_simplified.model_loader] load_model_and_tokenizer:89 - Model size: 2.61B parameters
2025-11-23 16:25:58,452 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:529 - Orthogonalizing weights to remove correct feature...
2025-11-23 16:25:58,469 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:66 - Orthogonalized embeddings, change magnitude: 19.8750
2025-11-23 16:25:58,507 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:120 - Total weight change magnitude: 51.0566
2025-11-23 16:25:58,508 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:121 - Modified 53 weight matrices
2025-11-23 16:25:58,508 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:129 - Average attention change: 0.4863
2025-11-23 16:25:58,508 [INFO] [weight_orthogonalization] orthogonalize_gemma_weights:131 - Average MLP change: 0.7130
2025-11-23 16:25:58,508 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:537 - 
Testing on initially correct problems...
2025-11-23 16:31:23,130 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:624 - 
Skipping incorrect baseline test (minimal scientific value - removing correct feature shouldn't help incorrect problems)
2025-11-23 16:31:23,131 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:660 - 
Results for CORRECT orthogonalization:
2025-11-23 16:31:23,131 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:661 -   Corruption rate: 100.0% (25/25)
2025-11-23 16:31:23,131 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:662 -   Average similarity: 0.024
2025-11-23 16:31:23,131 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:663 -   Accidental corrections: 0/0
2025-11-23 16:31:23,131 [INFO] [phase5_3.weight_orthogonalizer] apply_correct_orthogonalization:664 -   Corruption p-value: 0.0000 (significant)
2025-11-23 16:31:23,428 [INFO] [phase5_3.weight_orthogonalizer] cleanup_all_checkpoints:234 - Cleaned up 2 checkpoint files
2025-11-23 16:31:23,428 [INFO] [phase5_3.weight_orthogonalizer] create_visualizations:675 - Creating visualizations...
2025-11-23 16:31:23,774 [INFO] [phase5_3.weight_orthogonalizer] create_visualizations:734 - Saved visualization to data/phase5_3_humaneval/visualizations/orthogonalization_effects.png
2025-11-23 16:31:23,775 [INFO] [phase5_3.weight_orthogonalizer] save_examples:738 - Saving example generations...
2025-11-23 16:31:23,775 [INFO] [phase5_3.weight_orthogonalizer] save_examples:776 - Saved examples to data/phase5_3_humaneval/examples
2025-11-23 16:31:23,777 [INFO] [phase5_3.weight_orthogonalizer] run:867 - 
============================================================
2025-11-23 16:31:23,777 [INFO] [phase5_3.weight_orthogonalizer] run:868 - PHASE 5.3 SUMMARY
2025-11-23 16:31:23,777 [INFO] [phase5_3.weight_orthogonalizer] run:869 - ============================================================
2025-11-23 16:31:23,777 [INFO] [phase5_3.weight_orthogonalizer] run:870 - Incorrect orthogonalization:
2025-11-23 16:31:23,777 [INFO] [phase5_3.weight_orthogonalizer] run:871 -   - Correction rate: 0.7%
2025-11-23 16:31:23,777 [INFO] [phase5_3.weight_orthogonalizer] run:872 -   - Preservation rate: 92.0%
2025-11-23 16:31:23,777 [INFO] [phase5_3.weight_orthogonalizer] run:873 - Correct orthogonalization:
2025-11-23 16:31:23,778 [INFO] [phase5_3.weight_orthogonalizer] run:874 -   - Corruption rate: 100.0%
2025-11-23 16:31:23,778 [INFO] [phase5_3.weight_orthogonalizer] run:875 -   - Similarity score: 0.024
2025-11-23 16:31:23,778 [INFO] [phase5_3.weight_orthogonalizer] run:876 - Runtime: 2481.0 seconds
2025-11-23 16:31:23,778 [INFO] [phase5_3.weight_orthogonalizer] run:877 - Results saved to: data/phase5_3_humaneval
2025-11-23 16:31:23,778 [INFO] [phase5_3.weight_orthogonalizer] run:878 - ============================================================
2025-11-23 16:31:23,778 [INFO] [main] run_phase5_3:1037 - 
âœ… Phase 5.3 completed successfully
