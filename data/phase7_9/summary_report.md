# PVA Feature Universality Analysis Report

**Generated:** 2025-09-12T16:58:01.039969

---

## Executive Summary

This analysis examines whether Program Validity Awareness (PVA) features discovered in the base Gemma-2B model transfer to the instruction-tuned Gemma-2B-IT variant. The results indicate **LIMITED transferability** of PVA features across model architectures.

## 1. Baseline Performance Comparison

### Temperature 0.0 (Deterministic Generation)

| Model | Pass Rate | Correct | Incorrect | Total |
|-------|-----------|---------|-----------|-------|
| **Gemma-2B (Base)** | 29.90% | 116 | 272 | 388 |
| **Gemma-2B-IT** | 38.40% | 149 | 239 | 388 |
| **Improvement** | +8.51% | +33 | -33 | - |

**Key Finding:** The instruction-tuned model shows a 28.4% relative improvement in baseline performance.

## 2. Steering Effectiveness Analysis

### 2.1 Base Model (Phase 4.8)

- **Correction Rate:** 4.04% (incorrect → correct)
- **Corruption Rate:** 64.66% (correct → incorrect)
- **Preservation Rate:** 85.34%
- **Coefficients Used:** Correct=29, Incorrect=200

### 2.2 Instruction-Tuned Model (Phase 7.6)

- **Correction Rate:** 2.93% (incorrect → correct)
- **Corruption Rate:** 82.55% (correct → incorrect)
- **Preservation Rate:** 91.28%
- **Coefficients Used:** Correct=29, Incorrect=287

### 2.3 Cross-Model Comparison

| Metric | Base Model | Instruction-Tuned | Difference |
|--------|------------|-------------------|------------|
| Correction Rate | 4.04% | 2.93% | -1.12% |
| Corruption Rate | 64.66% | 82.55% | +17.90% |
| Preservation Rate | 85.34% | 91.28% | +91.28% |

## 3. Statistical Significance

### Base Model Statistical Tests

- **Correction:** p-value = 0.00e+00, Significant = True
- **Corruption:** p-value = 0.00e+00, Significant = True
- **Preservation:** p-value = 1.48e-15, Significant = True

### Instruction-Tuned Model Statistical Tests

- **Correction:** p-value = 0.00e+00, Significant = True
- **Corruption:** p-value = 0.00e+00, Significant = True
- **Preservation:** p-value = 2.59e-27, Significant = True

## 4. Universality Assessment

### Overall Verdict: **LIMITED**

| Criterion | Result | Interpretation |
|-----------|--------|----------------|
| Features Transfer | ✗ | PVA features do not transfer well |
| Correction Effective | ✗ | Steering ineffective for correction |
| Corruption Effective | ✓ | Steering can induce errors |
| Preservation Maintained | ✓ | Model can resist incorrect steering |

## 5. Key Findings and Implications

### Key Findings:

1. **Baseline Performance:** Instruction-tuning improves baseline pass rate by 8.51 percentage points (28.5% relative improvement)
2. **Correction Capability:** Slightly reduced in instruction-tuned model (2.93% vs 4.04%)
3. **Corruption Susceptibility:** Significantly increased in instruction-tuned model (82.55% vs 64.66%)
4. **Preservation Ability:** Dramatically improved in instruction-tuned model (91.28% vs 0%)
5. **Feature Transfer:** Limited - PVA features discovered in base model do not transfer effectively

### Research Implications:

- **Model-Specific Features:** PVA features appear to be architecture-dependent
- **Instruction-Tuning Impact:** Changes internal representations significantly
- **Steering Asymmetry:** Instruction-tuned models more resistant to correction but more vulnerable to corruption
- **Future Work:** Need separate feature discovery for each model variant

## 6. Experimental Details

- **Base Model:** google/gemma-2-2b
- **Instruction-Tuned Model:** google/gemma-2-2b-it
- **Dataset:** MBPP validation split (388 problems)
- **Temperature:** 0.0 (deterministic generation)
- **Phases Analyzed:** 3.5, 4.8, 7.3, 7.6

## 7. Conclusion

The analysis reveals that PVA features discovered through SAE analysis in the base Gemma-2B model exhibit **limited transferability** to the instruction-tuned Gemma-2B-IT variant. While both correction and corruption steering show statistically significant effects in both models, the patterns differ substantially. The instruction-tuned model shows improved resistance to incorrect steering (preservation) but increased vulnerability to corruption, suggesting fundamental differences in how program validity is represented internally. These findings indicate that interpretability insights may be model-specific and that instruction-tuning significantly alters the internal feature landscape relevant to code generation tasks.

---

*This report was automatically generated by universality_analysis.py*