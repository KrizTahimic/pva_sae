\begin{MintedVerbatim}[commandchars=\\\{\}]
\PYG{k}{class}\PYG{+w}{ }\PYG{n+nc}{AttentionExtractor}\PYG{p}{:}
    \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{\PYGZus{}create\PYGZus{}hook}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{layer\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{hook\PYGZus{}fn}\PYG{p}{(}\PYG{n}{module}\PYG{p}{,} \PYG{n+nb}{input}\PYG{p}{,} \PYG{n}{output}\PYG{p}{)}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} output.attentions: [batch, n\PYGZus{}heads, seq\PYGZus{}len, seq\PYGZus{}len]}
            \PYG{c+c1}{\PYGZsh{} Extract attention from last prompt token}
            \PYG{n}{attn} \PYG{o}{=} \PYG{n}{output}\PYG{o}{.}\PYG{n}{attentions}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} [batch, n\PYGZus{}heads, seq\PYGZus{}len]}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{attention\PYGZus{}patterns}\PYG{p}{[}\PYG{n}{layer\PYGZus{}idx}\PYG{p}{]} \PYG{o}{=} \PYG{n}{attn}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{hook\PYGZus{}fn}
\end{MintedVerbatim}
