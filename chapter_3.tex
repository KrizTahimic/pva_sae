\chapter{Methodology}
\label{sec:methodology}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{figures/methodology.pdf}
    \caption{Overview of the three-component methodology: dataset building, SAE analysis, and validation}
    \label{fig:methodology}
\end{figure}

% Adapts
This chapter outlines the methodological approach used in our study, drawing significant inspiration from the work of \citeA{ferrando2024know} in their investigation of entity recognition latent in language models. We adapt their framework while introducing key modifications to address our distinct research objectives.

% Three main parts
Our methodology consists of three main components: dataset building to collect and categorize program solutions, SAE activation analysis to identify latent direction, and validation to verify the found program validity awareness latent direction, as illustrated in Figure~\ref{fig:methodology}.

\section{Dataset Building}

This research will use the Mostly Basic Programming Problems (MBPP) dataset as prompts, which consists of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. For standardized prompting, we construct a template by concatenating three essential components:  problem description, 3 test cases, and code initiator (\texttt{\# Your code here}), as illustrated in Figure~\ref{fig:template}. 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{figures/template.pdf}
    \caption{Standardized prompt template containing three components: problem description, test cases, and code initiator.}
    \label{fig:template}
\end{figure}

The problem description provides the fundamental problem statement, while test cases lay out the function signature and the concrete demonstrations of the expected input-output behavior. Finally, the code initiator is a vital component in generating the code solution as we are prompting a base model, not an instruct model.

We then utilize Google's Gemma 2 language model with 2 billion parameters in its base configuration (\texttt{google/gemma-2-2b}) \cite{team2024gemma}. To ensure consistency and reproducibility in our code generation process, we set the temperature parameter to 0, prioritizing deterministic outputs over creative variations. This choice lets us focus on understanding the model's base behavior in code generation tasks.

Afterward, we categorize each sample based on the performance of the generated solution against the MBPP test cases. We classify solutions as correct when they pass all three test cases on the first attempt (pass@1). Any implementation that fails to achieve this criterion, regardless of the failure type (compilation errors, runtime exceptions, or incorrect outputs), is classified as incorrect.

Finally, we partition the dataset into SAE analysis (50\%), hyperparameter tuning (10\%), and validation (40\%) sets to enable robust model analysis and evaluation. The hyperparameter tuning set will be used to decide the threshold most applicable for F1 and the appropriate steering coefficient for model steering. 

\section{SAE Analysis}

To analyze program validity awareness latent direction in language models, we use already trained Sparse Autoencoders (SAEs) from GemmaScope \cite{lieberum2024gemma} to project model representations into a higher-dimensional space. This section details our analytical approach to identifying latent directions that react to the program validity awareness dataset.

\subsection{SAE Architecture}

We utilize the JumpReLU SAE architecture, which projects model representations $\mathbf{x} \in \mathbb{R}^d$ into a larger dimensional space $a(\mathbf{x}) \in \mathbb{R}^{d_{\text{SAE}}}$. The encoding process consists of a linear transformation followed by a JumpReLU activation function:

\begin{equation}
    a(\mathbf{x})=\text{JumpReLU}_{\theta}(\mathbf{xW}_{\text{enc}}+\mathbf{b}_{\text{enc}})
\end{equation}

where JumpReLU applies a threshold activation defined as:

\begin{equation}
    \text{JumpReLU}_\theta(\mathbf{x}) = \mathbf{x} \odot H(\mathbf{x} - \theta)
\end{equation}

Here, $H$ represents the Heaviside step function, and $\theta$ is a learnable threshold vector. The decoder reconstructs the input through:

\begin{equation}
    \text{SAE}(\mathbf{x})=a(\mathbf{x})\mathbf{W}_{\text{dec}}+\mathbf{b}_{\text{dec}}
\end{equation}

The autoencoder is trained to minimize a combined loss function incorporating reconstruction error and sparsity:

\begin{equation}
    \mathcal{L}(\mathbf{x}) = \underbrace{\|\mathbf{x} - \text{SAE}(\mathbf{x})\|_2^2}_{\mathcal{L}_{\text{reconstruction}}} + \underbrace{\lambda \|a(\mathbf{x})\|_0}_{\mathcal{L}_{\text{sparsity}}}
\end{equation}

\subsection{Separation Score Analysis}

We analyze the residual stream at the final token for each code sample in our dataset to capture the model's representation during the input phase when the model consumes the programming problem prompt. 

To ensure latent dimensions specifically correlate with code-related features rather than general language patterns, we exclude latent dimensions that activate frequently ($>2\%$) on tokens obtained from the Pile dataset. The remaining dimensions with the highest separation scores are then selected for further analysis and validation as potential indicators of program validity awareness.

We then compute activation statistics for each latent dimension across correct and incorrect code samples. For a given layer $l$ and latent dimension $j$, we calculate the fraction of activations on correct and incorrect code, respectively:

\begin{align}
    f_{l,j}^{\text{correct}} = \frac{\sum_{i}^{N^{\text{correct}}} \mathbf{1}[a_{l,j}(\mathbf{x}_{l,i}^{\text{correct}}) > 0]}{N^{\text{correct}}}, \quad   
    f_{l,j}^{\text{incorrect}} = \frac{\sum_{i}^{N^{\text{incorrect}}} \mathbf{1} [a_{l,j}(\mathbf{x}_{l,i}^{\text{incorrect}}) > 0]}{N^{\text{incorrect}}}
\end{align}

To identify latent dimensions that effectively distinguish between correct and incorrect code, we compute separation scores:

\begin{equation}
    s_{l,j}^{\text{correct}} = f_{l,j}^{\text{correct}} - f_{l,j}^{\text{incorrect}}, \qquad
    s_{l,j}^{\text{incorrect}} = f_{l,j}^{\text{incorrect}} - f_{l,j}^{\text{correct}}
\end{equation}


To identify robust latent direction detection for Python3, we utilize a $\arg\max$ operation for each layer, selecting the latents with the highest separation scores:

\begin{align}
\text{correct code direction} = \arg\max_{l,j} s_{l,j}^{\text{correct}} \\
\text{incorrect code direction} = \arg\max_{l,j} s_{l,j}^{\text{incorrect}}
\end{align}

The identified correct and incorrect code directions are the Program Validity Awareness Latent Directions which will be validated and steered.

\section{Validation}
\subsection{Statistical Analysis}
To validate the identified program validity awareness latent direction of the topmost correct code latent and incorrect code latent, we employ two statistical measures that assess different aspects of the latent direction's effectiveness and generalizability. 

% AUROC curve
First, we evaluate the latent direction's statistical significance using the Area Under the Receiver Operating Characteristic (AUROC) curve. This metric comprehensively assesses the model's ability to distinguish between correct and incorrect code implementations across various classification thresholds. The AUROC analysis offers insights into the latent direction's robustness and reliability across different decision boundaries. The AUROC score is calculated as follows:

\begin{equation} 
\text{AUROC} = \int_0^1 \text{TPR}(\text{FPR})\,d\text{FPR}
\end{equation}

The integral formulation captures the relationship between true and false positive rates across all possible classification thresholds. The True Positive Rate (TPR) measures the proportion of correct code implementations that are accurately identified (TP/(TP + FN)), while the False Positive Rate (FPR) indicates the proportion of incorrect implementations mistakenly classified as correct (FP/(FP + TN)). This mathematical framework quantifies the model's discriminative capability independent of any specific threshold, where a score closer to 1.0 indicates superior discrimination ability, and 0.5 suggests performance equivalent to random chance.


% F1 score
Second, we compute the F1 score, which provides a balanced measure between precision and True Positive Rate (TPR) in identifying program validity. The F1 score is calculated as the harmonic mean:

\begin{equation}
F_1 = \frac{2 \cdot \text{precision} \cdot \text{TPR}}{\text{precision} + \text{TPR}}
\end{equation}

where precision = TP/(TP + FP) represents the proportion of correctly identified valid programs among all programs classified as valid. The multiplication by 2 in the numerator and the division by the sum in the denominator creates a harmonic mean, ensuring equal weighting between precision and TPR. 

To compute the F1 score, we must first determine an optimal classification threshold for our classifier that converts our continuous model outputs into binary decisions of valid or invalid programs. We use our hyperparameter tuning set to evaluate multiple threshold values, calculating each threshold's resulting TP, FP, TN, and FN counts. These counts produce different precision and TPR values, yielding different F1 scores. We select the threshold that maximizes the F1 score on the development set and then apply this optimized threshold to our validation dataset for final evaluation. The resulting F1 score ranges from 0 to 1, where 1 indicates perfect precision and recall (TPR), while 0 indicates complete failure in either metric. This systematic threshold optimization ensures that our latent direction maintains high accuracy while minimizing misclassifications in both directions, providing a comprehensive metric that captures the latent direction's overall performance in distinguishing between valid and invalid programs.


\subsection{Robustness Analysis}

To ensure the reliability and generalizability of our findings, we conduct two types of robustness analyses to evaluate the stability of the identified program validity awareness latent directions under varying conditions.

% Temperature Variation Analysis
First, the Temperature Variation Analysis examines how varying temperature affects the statistical significance of our program validity awareness latent directions. While our primary analysis uses deterministic sampling (temperature = 0), we systematically evaluate the model's performance across a range of temperature values: 0.0, 0.3, 0.6, 0.9, and 1.2. For each temperature value, we generate five distinct code solutions for every problem in the validation set. The AUROC metric is then computed for each temperature setting, allowing us to visualize how sampling variability affects the statistical significance of the PVA latent directions.

% Difficulty Variation Analysis
Second, the Difficulty Variation Analysis examines how the identified latent directions generalize across problems of varying difficulty levels. Using the randon cyclomatic complexity, we compute the AUROC metric for each difficulty score. This analysis allows us to assess whether the program validity awareness mechanism operates uniformly across problem complexities or exhibits differential effectiveness based on problem difficulty.

\subsection{Model Steering}

To validate our discovery of program validity awareness latent in the model's representations, we employ a technique called model steering using the identified Sparse Autoencoder (SAE) latents. This approach leverages the fundamental property of SAEs: their ability to reconstruct a model's internal representations as interpretable latent directions. Mathematically, an SAE reconstructs a model's representation $\mathbf{x}$ as a combination of learned latent directions: $\mathbf{x} \approx a(\mathbf{x})\mathbf{W}_{\text{dec}} + \mathbf{b}_{\text{dec}}$.
This reconstruction can be understood more intuitively by expanding it into its components: $\mathbf{x} \approx \sum_j a_j(\mathbf{x})\mathbf{W}_{\text{dec}}[j,:]$. Here, each row of the decoder matrix $\mathbf{W}_{\text{dec}}$ represents a learned latent direction, and $a_j(\mathbf{x})$ represents how strongly that latent direction is present in the input. Modifying these activation values allows us to control how much each latent direction contributes to the model's processing.
Building on this understanding, we can steer the model's behavior by adjusting the activation value of specific SAE latent directions. This process, known as model steering, involves updating the model's residual stream according to the equation:

\begin{equation}
\mathbf{x}^{\text{new}} \leftarrow \mathbf{x} + \alpha\mathbf{d}_j
\end{equation}

where $\alpha$ controls the strength of the intervention and $\mathbf{d}_j$ is the latent direction we wish to accentuate. This allows us to systematically test how the identified program validity awareness latent directions influence the model's behavior, validating our findings empirically.

To determine an appropriate steering coefficient $\alpha$, we conduct empirical testing on a hyperparameter tuning set with various coefficient values ranging from 0 to 1200. We observe that values below \_\_\_ produce an inconsistent flip rate, while values above \_\_\_ lead to unintelligible output quality with formatting issues and incoherent responses. We select $\alpha = \_\_\_$, which consistently produces a clear code correction or corruption while maintaining coherent text generation. This range corresponds to approximately twice the typical residual stream norm in relevant model layers.

To quantify the impact of our model steering, we introduce two flip rate metrics: the Correction Rate and the Corruption Rate. For the correct latent direction, we measure the Correction Rate, which quantifies the proportion of initially incorrect code samples that become correct after steering:
\begin{equation}
\text{Correction Rate} = \frac{1}{N^{\text{incorrect}}} \sum_{i=1}^{N^{\text{incorrect}}} \mathbf{1}[\text{IsCorrect}(\text{steered}_i)= \text{True}]
\end{equation}

For the incorrect latent direction, we measure the Corruption Rate, which captures the proportion of initially correct code that becomes incorrect after steering:
\begin{equation}
\text{Corruption Rate} = \frac{1}{N^{\text{correct}}} \sum_{i=1}^{N^{\text{correct}}} \mathbf{1}[\text{IsCorrect}(\text{steered}_i)= \text{False}]
\end{equation}

To determine whether these observed rates represent statistically significant effects, we apply Binomial Testing to each rate. For a given test with $n$ trials, $k$ successes, and baseline probability $p_0$, the probability of observing $k$ or more successes under the null hypothesis is:
\begin{equation}
P(X \geq k) = \sum_{i=k}^{n} \binom{n}{i} p_0^i (1-p_0)^{n-i}
\end{equation}

For the correct latent direction, we test:
\begin{align}
H_0&: \text{Correction Rate} = p_0 \\
H_1&: \text{Correction Rate} > p_0
\end{align}

Similarly, for the incorrect latent direction, we test:
\begin{align}
H_0&: \text{Corruption Rate} = p_0 \\
H_1&: \text{Corruption Rate} > p_0
\end{align}

To establish valid baseline flip rates ($p_0$) for our statistical tests, we steer latent directions picked from the same layer of program validity awareness latent direction through simple random sampling using the same steering coefficient $\alpha$. We apply these control interventions to both initially correct and incorrect samples to measure the background rate of state changes that occur due to non-specific perturbations. These baseline rates serve as our null hypothesis values, allowing us to determine whether our targeted latent direction interventions produce statistically significant effects beyond what would be expected from arbitrary model steering.  We reject the null hypothesis when p-values fall below 0.05, allowing us to conclude with statistical confidence that our identified latent directions causally influence code correctness.
