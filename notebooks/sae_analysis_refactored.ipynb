{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Analysis for PVA-SAE Project\n",
    "\n",
    "This notebook implements the core SAE analysis pipeline:\n",
    "1. **SAE Activation Collection** - Extract activations at final token using hooks\n",
    "2. **Compute Separation Scores** - Calculate activation fractions per thesis methodology  \n",
    "3. **ArgMax Selection** - Find best features for correct/incorrect code\n",
    "4. **PVA Latent Direction** - Identify program validity awareness directions\n",
    "\n",
    "Using: Gemma 2 2B model, Layer 20 SAE, 10 MBPP samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Setup\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir = Path(\"../data/datasets\")\n",
    "dataset_files = list(data_dir.glob(\"dataset_*.parquet\"))\n",
    "\n",
    "if dataset_files:\n",
    "    latest_dataset = max(dataset_files, key=lambda x: x.stat().st_mtime)\n",
    "    df = pd.read_parquet(latest_dataset)\n",
    "    print(f\"Loaded: {latest_dataset.name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    # Add is_correct column\n",
    "    if 'test_passed' in df.columns:\n",
    "        df['is_correct'] = df['test_passed']\n",
    "    \n",
    "    print(f\"\\nCorrect: {df['is_correct'].sum()}, Incorrect: {(~df['is_correct']).sum()}\")\n",
    "else:\n",
    "    print(\"No dataset found!\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct prompts using shared template\n",
    "from phase2_sae_analysis.prompt_utils import build_prompt_template\n",
    "from phase1_dataset_building.dataset_manager import PromptAwareDatasetManager\n",
    "\n",
    "if df is not None and 'prompt' not in df.columns:\n",
    "    print(\"Reconstructing prompts...\")\n",
    "    \n",
    "    # Load MBPP dataset\n",
    "    dataset_manager = PromptAwareDatasetManager()\n",
    "    dataset_manager.load_dataset()\n",
    "    \n",
    "    # Reconstruct prompts\n",
    "    prompts = []\n",
    "    for task_id in df['task_id']:\n",
    "        record = dataset_manager.get_record(task_id)\n",
    "        \n",
    "        # Extract components\n",
    "        problem_desc = record['text'].strip()\n",
    "        test_cases = '\\n'.join(record['test_list'])\n",
    "        \n",
    "        # Build prompt using shared template\n",
    "        prompt = build_prompt_template(problem_desc, test_cases)\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    df['prompt'] = prompts\n",
    "    print(f\"Reconstructed {len(prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and SAE configuration\n",
    "MODEL_NAME = \"google/gemma-2-2b\"\n",
    "SAE_LAYER = 20\n",
    "SAE_CONFIG = {\n",
    "    \"repo_id\": \"google/gemma-scope-2b-pt-res\",\n",
    "    \"sae_id\": f\"layer_{SAE_LAYER}/width_16k/average_l0_71\",\n",
    "}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"SAE Layer: {SAE_LAYER}\")\n",
    "print(f\"SAE: {SAE_CONFIG['repo_id']}/{SAE_CONFIG['sae_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SAE Activation Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SAE analyzer functions\n",
    "from phase2_sae_analysis.sae_analyzer import (\n",
    "    load_gemma_scope_sae,\n",
    "    ActivationExtractor,\n",
    "    compute_separation_scores,\n",
    "    PVALatentDirection\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and SAE\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    ")\n",
    "if DEVICE == \"mps\":\n",
    "    model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Model loaded on {DEVICE}\")\n",
    "\n",
    "# Load SAE\n",
    "sae_model = load_gemma_scope_sae(\n",
    "    repo_id=SAE_CONFIG[\"repo_id\"],\n",
    "    sae_id=SAE_CONFIG[\"sae_id\"],\n",
    "    device=DEVICE\n",
    ")\n",
    "print(\"SAE loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect activations\n",
    "if df is not None:\n",
    "    print(f\"Extracting activations for {len(df)} samples...\")\n",
    "    \n",
    "    # Prepare prompts (last token of prompt before generation)\n",
    "    prompts = df['prompt'].tolist()\n",
    "    \n",
    "    # Extract activations\n",
    "    extractor = ActivationExtractor(model, tokenizer, DEVICE)\n",
    "    \n",
    "    all_activations = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\rProcessing {i+1}/{len(prompts)}\", end=\"\")\n",
    "        acts = extractor.extract_activations([prompt], SAE_LAYER)\n",
    "        all_activations.append(acts[0])  # Get first (only) sample\n",
    "    \n",
    "    print(\"\\nActivations extracted\")\n",
    "    \n",
    "    # Apply SAE encoding\n",
    "    print(\"Applying SAE encoding...\")\n",
    "    sae_activations = []\n",
    "    with torch.no_grad():\n",
    "        for act in all_activations:\n",
    "            sae_act = sae_model.encode(act.unsqueeze(0)).squeeze(0)\n",
    "            sae_activations.append(sae_act)\n",
    "    \n",
    "    # Split by correctness\n",
    "    correct_activations = [sae_activations[i] for i in range(len(df)) if df.iloc[i]['is_correct']]\n",
    "    incorrect_activations = [sae_activations[i] for i in range(len(df)) if not df.iloc[i]['is_correct']]\n",
    "    \n",
    "    print(f\"SAE activations: {len(correct_activations)} correct, {len(incorrect_activations)} incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Separation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute separation scores\n",
    "if 'correct_activations' in locals() and 'incorrect_activations' in locals():\n",
    "    print(\"Computing separation scores...\")\n",
    "    \n",
    "    scores = compute_separation_scores(\n",
    "        torch.stack(correct_activations),\n",
    "        torch.stack(incorrect_activations)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nActivation fraction statistics:\")\n",
    "    print(f\"  Mean f_correct: {scores.f_correct.mean():.4f}\")\n",
    "    print(f\"  Mean f_incorrect: {scores.f_incorrect.mean():.4f}\")\n",
    "    print(f\"  Max s_correct: {scores.s_correct.max():.4f}\")\n",
    "    print(f\"  Max s_incorrect: {scores.s_incorrect.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ArgMax Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best PVA directions\n",
    "if 'scores' in locals():\n",
    "    # Create PVA directions\n",
    "    correct_direction = PVALatentDirection(\n",
    "        direction_type=\"correct\",\n",
    "        layer=SAE_LAYER,\n",
    "        feature_idx=scores.best_correct_idx,\n",
    "        separation_score=scores.s_correct[scores.best_correct_idx].item(),\n",
    "        f_correct=scores.f_correct[scores.best_correct_idx].item(),\n",
    "        f_incorrect=scores.f_incorrect[scores.best_correct_idx].item()\n",
    "    )\n",
    "    \n",
    "    incorrect_direction = PVALatentDirection(\n",
    "        direction_type=\"incorrect\",\n",
    "        layer=SAE_LAYER,\n",
    "        feature_idx=scores.best_incorrect_idx,\n",
    "        separation_score=scores.s_incorrect[scores.best_incorrect_idx].item(),\n",
    "        f_correct=scores.f_correct[scores.best_incorrect_idx].item(),\n",
    "        f_incorrect=scores.f_incorrect[scores.best_incorrect_idx].item()\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PVA LATENT DIRECTIONS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\n{correct_direction}\")\n",
    "    print(f\"  Activates on {correct_direction.f_correct:.1%} of correct, {correct_direction.f_incorrect:.1%} of incorrect\")\n",
    "    print(f\"\\n{incorrect_direction}\")\n",
    "    print(f\"  Activates on {incorrect_direction.f_correct:.1%} of correct, {incorrect_direction.f_incorrect:.1%} of incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visualization of top features\n",
    "if 'scores' in locals():\n",
    "    # Get top 10 features for each direction\n",
    "    top_k = 10\n",
    "    \n",
    "    top_correct_scores, top_correct_idx = torch.topk(scores.s_correct, top_k)\n",
    "    top_incorrect_scores, top_incorrect_idx = torch.topk(scores.s_incorrect, top_k)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Correct direction\n",
    "    ax1.bar(range(top_k), top_correct_scores.cpu().numpy())\n",
    "    ax1.set_xlabel('Rank')\n",
    "    ax1.set_ylabel('Separation Score (f_correct - f_incorrect)')\n",
    "    ax1.set_title('Top Correct Code Features')\n",
    "    ax1.set_xticks(range(top_k))\n",
    "    ax1.set_xticklabels([f'{idx}' for idx in top_correct_idx.cpu().numpy()], rotation=45)\n",
    "    \n",
    "    # Incorrect direction\n",
    "    ax2.bar(range(top_k), top_incorrect_scores.cpu().numpy(), color='orange')\n",
    "    ax2.set_xlabel('Rank')\n",
    "    ax2.set_ylabel('Separation Score (f_incorrect - f_correct)')\n",
    "    ax2.set_title('Top Incorrect Code Features')\n",
    "    ax2.set_xticks(range(top_k))\n",
    "    ax2.set_xticklabels([f'{idx}' for idx in top_incorrect_idx.cpu().numpy()], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    elif DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Memory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "1. ✅ Collected SAE activations at the final prompt token\n",
    "2. ✅ Computed separation scores using activation fractions (thesis methodology)\n",
    "3. ✅ Selected best features via argmax for both correct/incorrect directions\n",
    "4. ✅ Identified PVA latent directions for layer 20\n",
    "\n",
    "The identified directions can be used for model steering in Phase 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}