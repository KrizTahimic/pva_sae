{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Analysis Exploration\n",
    "\n",
    "This notebook explores Sparse Autoencoder (SAE) analysis for the PVA-SAE project.\n",
    "We'll analyze how language models internally represent code correctness using GemmaScope SAEs.\n",
    "\n",
    "## Methodology Overview\n",
    "1. Load generated dataset (correct vs incorrect solutions)\n",
    "2. Extract model activations using GemmaScope SAEs  \n",
    "3. Compute separation scores for latent dimensions\n",
    "4. Filter out general language patterns (>2% activation on Pile dataset)\n",
    "5. Identify distinguishing latent directions for code correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"📚 Imports completed successfully!\")\n",
    "print(f\"🔧 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🐼 Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Generated Dataset\n",
    "\n",
    "Load the dataset generated from Phase 1 (code generation and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest dataset file\n",
    "data_dir = Path(\"data/datasets\")\n",
    "dataset_files = list(data_dir.glob(\"dataset_*.parquet\"))\n",
    "\n",
    "if dataset_files:\n",
    "    # Get the most recent dataset\n",
    "    latest_dataset = max(dataset_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"📊 Loading dataset: {latest_dataset.name}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_parquet(latest_dataset)\n",
    "    print(f\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"📏 Dataset shape: {df.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No dataset files found in data/datasets/\")\n",
    "    print(\"💡 Generate a test dataset first with:\")\n",
    "    print(\"   python3 run.py phase 1 --model google/gemma-2-9b --start 0 --end 9\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "if df is not None:\n",
    "    print(\"📋 Dataset Overview:\")\n",
    "    print(f\"   Total records: {len(df)}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    print(\"\\n📊 Correctness Distribution:\")\n",
    "    correctness_counts = df['is_correct'].value_counts()\n",
    "    print(f\"   Correct solutions: {correctness_counts.get(True, 0)}\")\n",
    "    print(f\"   Incorrect solutions: {correctness_counts.get(False, 0)}\")\n",
    "    print(f\"   Success rate: {df['is_correct'].mean()*100:.1f}%\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\n🔍 Sample Data:\")\n",
    "    display(df[['task_id', 'is_correct', 'passed_tests', 'total_tests', 'generation_time']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Load the same model used for generation to extract activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Model configuration (should match the one used in dataset generation)\n",
    "MODEL_NAME = \"google/gemma-2-9b\"  # Update this if using different model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"🤖 Loading model: {MODEL_NAME}\")\n",
    "print(f\"🔧 Device: {DEVICE}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"✅ Tokenizer loaded successfully!\")\n",
    "\n",
    "# Load model (we'll load this when we need activations)\n",
    "model = None  # Load on demand to save memory\n",
    "print(\"⏳ Model will be loaded on demand for activation extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GemmaScope SAE Setup\n",
    "\n",
    "Set up GemmaScope Sparse Autoencoders for analyzing model activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GemmaScope SAE configuration\n",
    "# Note: Update these URLs/paths based on actual GemmaScope availability\n",
    "\n",
    "SAE_CONFIG = {\n",
    "    \"model_name\": \"gemma-2-9b\",\n",
    "    \"layer\": 41,  # Final layer for residual stream analysis\n",
    "    \"sae_type\": \"JumpReLU\",  # GemmaScope uses JumpReLU architecture\n",
    "    \"width\": 65536,  # SAE width\n",
    "    \"activation_threshold\": 0.05,  # Minimum activation threshold\n",
    "}\n",
    "\n",
    "print(\"⚙️ SAE Configuration:\")\n",
    "for key, value in SAE_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Placeholder for SAE loading\n",
    "# TODO: Replace with actual GemmaScope SAE loading code\n",
    "print(\"\\n📝 Note: SAE loading code needs to be implemented based on GemmaScope API\")\n",
    "print(\"🔗 Refer to: https://github.com/google-deepmind/gemma_scope\")\n",
    "\n",
    "sae_model = None  # Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Model Activations\n",
    "\n",
    "Extract activations at the final token position for each generated solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(text: str, model, tokenizer, layer_idx: int = -1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract activations from model at specified layer for final token position\n",
    "    \n",
    "    Args:\n",
    "        text: Input text (prompt + generated code)\n",
    "        model: Language model\n",
    "        tokenizer: Model tokenizer\n",
    "        layer_idx: Layer index (-1 for final layer)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Activations at final token position\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Extract activations with no gradient computation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        # Get activations from specified layer at final token position\n",
    "        hidden_states = outputs.hidden_states[layer_idx]  # Shape: (batch, seq_len, hidden_dim)\n",
    "        final_token_activations = hidden_states[0, -1, :]  # Shape: (hidden_dim,)\n",
    "        \n",
    "    return final_token_activations.cpu()\n",
    "\n",
    "# Test activation extraction (when model is loaded)\n",
    "if model is not None and df is not None and len(df) > 0:\n",
    "    sample_text = df.iloc[0]['prompt'] + df.iloc[0]['generated_code']\n",
    "    print(f\"🧪 Testing activation extraction on sample text...\")\n",
    "    activations = extract_activations(sample_text, model, tokenizer)\n",
    "    print(f\"✅ Extracted activations shape: {activations.shape}\")\nelse:\n",
    "    print(\"⏳ Activation extraction will be tested when model and data are loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE Activation Analysis\n",
    "\n",
    "Apply SAEs to extract sparse representations and analyze patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sae(activations: torch.Tensor, sae_model) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply SAE to extract sparse representation\n",
    "    \n",
    "    Args:\n",
    "        activations: Raw model activations\n",
    "        sae_model: Trained SAE model\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Sparse SAE activations\n",
    "    \"\"\"\n",
    "    # TODO: Implement based on GemmaScope SAE API\n",
    "    # This is a placeholder implementation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Apply SAE encoder\n",
    "        sparse_activations = sae_model.encode(activations)\n",
    "        \n",
    "        # Apply activation threshold\n",
    "        threshold = SAE_CONFIG[\"activation_threshold\"]\n",
    "        sparse_activations = torch.where(\n",
    "            sparse_activations > threshold, \n",
    "            sparse_activations, \n",
    "            torch.zeros_like(sparse_activations)\n",
    "        )\n",
    "        \n",
    "    return sparse_activations\n",
    "\n",
    "# Placeholder for SAE activation extraction\n",
    "print(\"📝 SAE activation extraction function defined\")\n",
    "print(\"🔧 Implementation pending GemmaScope SAE integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation Score Calculation\n",
    "\n",
    "Calculate separation scores to identify latent dimensions that distinguish correct vs incorrect solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_separation_scores(correct_activations: List[torch.Tensor], \n",
    "                              incorrect_activations: List[torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate separation scores for each SAE dimension\n",
    "    \n",
    "    Args:\n",
    "        correct_activations: List of activations for correct solutions\n",
    "        incorrect_activations: List of activations for incorrect solutions\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Separation scores for each dimension\n",
    "    \"\"\"\n",
    "    # Stack activations\n",
    "    correct_stack = torch.stack(correct_activations)  # Shape: (n_correct, sae_width)\n",
    "    incorrect_stack = torch.stack(incorrect_activations)  # Shape: (n_incorrect, sae_width)\n",
    "    \n",
    "    # Calculate means for each dimension\n",
    "    correct_means = correct_stack.mean(dim=0)  # Shape: (sae_width,)\n",
    "    incorrect_means = incorrect_stack.mean(dim=0)  # Shape: (sae_width,)\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    correct_stds = correct_stack.std(dim=0) + 1e-8  # Add small epsilon for numerical stability\n",
    "    incorrect_stds = incorrect_stack.std(dim=0) + 1e-8\n",
    "    \n",
    "    # Calculate pooled standard deviation\n",
    "    n_correct = len(correct_activations)\n",
    "    n_incorrect = len(incorrect_activations)\n",
    "    pooled_std = torch.sqrt(((n_correct - 1) * correct_stds**2 + \n",
    "                           (n_incorrect - 1) * incorrect_stds**2) / \n",
    "                          (n_correct + n_incorrect - 2))\n",
    "    \n",
    "    # Calculate separation score (Cohen's d)\n",
    "    separation_scores = torch.abs(correct_means - incorrect_means) / pooled_std\n",
    "    \n",
    "    return separation_scores\n",
    "\n",
    "def analyze_top_separating_dimensions(separation_scores: torch.Tensor, \n",
    "                                    top_k: int = 20) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze top separating dimensions\n",
    "    \n",
    "    Args:\n",
    "        separation_scores: Calculated separation scores\n",
    "        top_k: Number of top dimensions to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Get top separating dimensions\n",
    "    top_scores, top_indices = torch.topk(separation_scores, top_k)\n",
    "    \n",
    "    results = {\n",
    "        'top_indices': top_indices.tolist(),\n",
    "        'top_scores': top_scores.tolist(),\n",
    "        'total_dimensions': len(separation_scores),\n",
    "        'mean_separation': separation_scores.mean().item(),\n",
    "        'std_separation': separation_scores.std().item(),\n",
    "        'max_separation': separation_scores.max().item()\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"🧮 Separation score calculation functions defined\")\n",
    "print(\"📊 Ready to analyze latent dimension separability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pile Dataset Filtering\n",
    "\n",
    "Filter out dimensions that activate frequently on general language (Pile dataset) to focus on code-specific patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_general_language_patterns(separation_scores: torch.Tensor,\n",
    "                                    pile_activation_rates: Optional[torch.Tensor] = None,\n",
    "                                    pile_threshold: float = 0.02) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Filter out dimensions that activate frequently on general language\n",
    "    \n",
    "    Args:\n",
    "        separation_scores: Calculated separation scores\n",
    "        pile_activation_rates: Activation rates on Pile dataset (if available)\n",
    "        pile_threshold: Threshold for filtering (default 2%)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Filtered separation scores\n",
    "    \"\"\"\n",
    "    if pile_activation_rates is not None:\n",
    "        # Filter dimensions that activate > pile_threshold on Pile dataset\n",
    "        code_specific_mask = pile_activation_rates <= pile_threshold\n",
    "        filtered_scores = separation_scores * code_specific_mask.float()\n",
    "        \n",
    "        n_filtered = (~code_specific_mask).sum().item()\n",
    "        print(f\"🔍 Filtered out {n_filtered} dimensions (>{pile_threshold*100:.1f}% activation on Pile)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  Pile activation rates not available - using unfiltered scores\")\n",
    "        print(\"💡 Consider loading pre-computed Pile activation rates for better filtering\")\n",
    "        filtered_scores = separation_scores\n",
    "    \n",
    "    return filtered_scores\n",
    "\n",
    "# Placeholder for Pile dataset activation rates\n",
    "# TODO: Load pre-computed activation rates from GemmaScope\n",
    "pile_rates = None\n",
    "\n",
    "print(\"🔧 General language filtering function defined\")\n",
    "print(\"📝 Note: Pile activation rates need to be loaded from GemmaScope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "Functions for visualizing SAE analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separation_distribution(separation_scores: torch.Tensor, title: str = \"Separation Score Distribution\"):\n",
    "    \"\"\"\n",
    "    Plot distribution of separation scores\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(separation_scores.numpy(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Separation Score (Cohen\\'s d)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{title}\\nHistogram')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot log scale\n",
    "    plt.subplot(1, 2, 2)\n",
    "    non_zero_scores = separation_scores[separation_scores > 0]\n",
    "    plt.hist(non_zero_scores.numpy(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Separation Score (Cohen\\'s d)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Non-zero Scores Only')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_dimensions(analysis_results: Dict, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Plot top separating dimensions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    top_indices = analysis_results['top_indices'][:top_k]\n",
    "    top_scores = analysis_results['top_scores'][:top_k]\n",
    "    \n",
    "    # Bar plot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    bars = plt.bar(range(len(top_scores)), top_scores, alpha=0.8)\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Separation Score')\n",
    "    plt.title(f'Top {top_k} Separating Dimensions')\n",
    "    plt.xticks(range(len(top_scores)), [f'Dim {idx}' for idx in top_indices], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score) in enumerate(zip(bars, top_scores)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Summary statistics\n",
    "    plt.subplot(2, 1, 2)\n",
    "    stats_text = f\"\"\"\n",
    "    Analysis Summary:\n",
    "    • Total SAE dimensions: {analysis_results['total_dimensions']:,}\n",
    "    • Mean separation score: {analysis_results['mean_separation']:.3f}\n",
    "    • Std separation score: {analysis_results['std_separation']:.3f}\n",
    "    • Max separation score: {analysis_results['max_separation']:.3f}\n",
    "    • Top dimension index: {analysis_results['top_indices'][0]}\n",
    "    \"\"\"\n",
    "    plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_activation_comparison(correct_activations: List[torch.Tensor],\n",
    "                             incorrect_activations: List[torch.Tensor],\n",
    "                             dimension_idx: int):\n",
    "    \"\"\"\n",
    "    Compare activations for a specific dimension between correct and incorrect solutions\n",
    "    \"\"\"\n",
    "    correct_vals = [act[dimension_idx].item() for act in correct_activations]\n",
    "    incorrect_vals = [act[dimension_idx].item() for act in incorrect_activations]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Box plot comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    data_to_plot = [correct_vals, incorrect_vals]\n",
    "    box_plot = plt.boxplot(data_to_plot, labels=['Correct', 'Incorrect'], patch_artist=True)\n",
    "    box_plot['boxes'][0].set_facecolor('lightgreen')\n",
    "    box_plot['boxes'][1].set_facecolor('lightcoral')\n",
    "    plt.ylabel('Activation Value')\n",
    "    plt.title(f'Dimension {dimension_idx} Activation Comparison')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(correct_vals, alpha=0.6, label='Correct', color='green', bins=10)\n",
    "    plt.hist(incorrect_vals, alpha=0.6, label='Incorrect', color='red', bins=10)\n",
    "    plt.xlabel('Activation Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Dimension {dimension_idx} Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"📊 Visualization functions defined\")\n",
    "print(\"🎨 Ready to create SAE analysis plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main SAE Analysis Pipeline\n",
    "\n",
    "Run the complete SAE analysis pipeline on the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sae_analysis_pipeline(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Run complete SAE analysis pipeline\n",
    "    \n",
    "    Args:\n",
    "        df: Dataset with generated solutions\n",
    "        \n",
    "    Returns:\n",
    "        dict: Complete analysis results\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting SAE Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    correct_df = df[df['is_correct'] == True]\n",
    "    incorrect_df = df[df['is_correct'] == False]\n",
    "    \n",
    "    print(f\"📊 Data prepared:\")\n",
    "    print(f\"   Correct solutions: {len(correct_df)}\")\n",
    "    print(f\"   Incorrect solutions: {len(incorrect_df)}\")\n",
    "    \n",
    "    if len(correct_df) == 0 or len(incorrect_df) == 0:\n",
    "        print(\"❌ Need both correct and incorrect solutions for analysis\")\n",
    "        return {}\n",
    "    \n",
    "    # Step 2: Extract activations (placeholder)\n",
    "    print(\"\\n🧠 Extracting model activations...\")\n",
    "    print(\"⚠️  This step requires actual model and SAE loading\")\n",
    "    \n",
    "    # Placeholder: In real implementation, extract activations here\n",
    "    # correct_activations = [extract_sae_activations(row) for _, row in correct_df.iterrows()]\n",
    "    # incorrect_activations = [extract_sae_activations(row) for _, row in incorrect_df.iterrows()]\n",
    "    \n",
    "    # Mock data for demonstration\n",
    "    sae_width = SAE_CONFIG[\"width\"]\n",
    "    correct_activations = [torch.randn(sae_width) * 0.1 for _ in range(len(correct_df))]\n",
    "    incorrect_activations = [torch.randn(sae_width) * 0.1 for _ in range(len(incorrect_df))]\n",
    "    \n",
    "    # Add some signal to demonstrate separation\n",
    "    signal_dims = [100, 500, 1000, 2000, 5000]  # Mock signal dimensions\n",
    "    for act in correct_activations:\n",
    "        for dim in signal_dims:\n",
    "            act[dim] += torch.randn(1) * 0.2 + 0.5  # Add positive signal\n",
    "    \n",
    "    for act in incorrect_activations:\n",
    "        for dim in signal_dims:\n",
    "            act[dim] += torch.randn(1) * 0.2 - 0.3  # Add negative signal\n",
    "    \n",
    "    print(f\"✅ Mock activations generated (shape: {sae_width})\")\n",
    "    \n",
    "    # Step 3: Calculate separation scores\n",
    "    print(\"\\n🧮 Calculating separation scores...\")\n",
    "    separation_scores = calculate_separation_scores(correct_activations, incorrect_activations)\n",
    "    print(f\"✅ Separation scores calculated\")\n",
    "    \n",
    "    # Step 4: Filter general language patterns\n",
    "    print(\"\\n🔍 Filtering general language patterns...\")\n",
    "    filtered_scores = filter_general_language_patterns(separation_scores, pile_rates)\n",
    "    \n",
    "    # Step 5: Analyze top dimensions\n",
    "    print(\"\\n📊 Analyzing top separating dimensions...\")\n",
    "    analysis_results = analyze_top_separating_dimensions(filtered_scores, top_k=20)\n",
    "    \n",
    "    # Step 6: Create visualizations\n",
    "    print(\"\\n🎨 Creating visualizations...\")\n",
    "    plot_separation_distribution(filtered_scores, \"Filtered Separation Scores\")\n",
    "    plot_top_dimensions(analysis_results, top_k=10)\n",
    "    \n",
    "    # Analyze a specific top dimension\n",
    "    if analysis_results['top_indices']:\n",
    "        top_dim = analysis_results['top_indices'][0]\n",
    "        plot_activation_comparison(correct_activations, incorrect_activations, top_dim)\n",
    "    \n",
    "    print(\"\\n✅ SAE Analysis Pipeline Complete!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {\n",
    "        'separation_scores': separation_scores,\n",
    "        'filtered_scores': filtered_scores,\n",
    "        'analysis_results': analysis_results,\n",
    "        'correct_activations': correct_activations,\n",
    "        'incorrect_activations': incorrect_activations\n",
    "    }\n",
    "\n",
    "print(\"🔧 SAE analysis pipeline function defined\")\n",
    "print(\"📋 Ready to run analysis when dataset is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis\n",
    "\n",
    "Execute the SAE analysis on the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis if dataset is available\n",
    "if df is not None:\n",
    "    print(\"🚀 Running SAE Analysis Pipeline...\")\n",
    "    analysis_results = run_sae_analysis_pipeline(df)\n",
    "    \n",
    "    if analysis_results:\n",
    "        print(\"\\n🎉 Analysis completed successfully!\")\n",
    "        print(\"📊 Results available in 'analysis_results' variable\")\n",
    "    else:\n",
    "        print(\"❌ Analysis failed - check data requirements\")\nelse:\n",
    "    print(\"❌ No dataset loaded. Please generate a dataset first:\")\n",
    "    print(\"   python3 run.py phase 1 --model google/gemma-2-9b --start 0 --end 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and TODOs\n",
    "\n",
    "### Implementation TODOs:\n",
    "\n",
    "1. **GemmaScope Integration**\n",
    "   - Load actual GemmaScope SAE models\n",
    "   - Implement SAE activation extraction\n",
    "   - Load pre-computed Pile activation rates\n",
    "\n",
    "2. **Model Loading**\n",
    "   - Load the actual language model for activation extraction\n",
    "   - Implement efficient activation caching\n",
    "   - Handle memory management for large models\n",
    "\n",
    "3. **Enhanced Analysis**\n",
    "   - Implement statistical significance testing\n",
    "   - Add more sophisticated filtering methods\n",
    "   - Compute AUROC and F1 scores for validation\n",
    "\n",
    "4. **Integration with Main Codebase**\n",
    "   - Migrate working code to `phase2_sae_analysis/sae_analyzer.py`\n",
    "   - Add configuration management\n",
    "   - Implement proper logging and error handling\n",
    "\n",
    "### Expected Outcomes:\n",
    "- Identification of SAE dimensions that distinguish code correctness\n",
    "- Separation scores > 0.5 for meaningful latent directions\n",
    "- Filtered dimensions specific to code patterns (not general language)\n",
    "- Foundation for Phase 3 model steering experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of current state\n",
    "print(\"📋 Current Implementation Status:\")\n",
    "print(\"✅ Dataset loading and exploration\")\n",
    "print(\"✅ Separation score calculation\")\n",
    "print(\"✅ Visualization functions\")\n",
    "print(\"✅ Analysis pipeline structure\")\n",
    "print(\"⏳ GemmaScope SAE integration (pending)\")\n",
    "print(\"⏳ Actual model activation extraction (pending)\")\n",
    "print(\"⏳ Pile dataset filtering (pending)\")\n",
    "\n",
    "print(\"\\n🚀 Ready for GemmaScope integration and real data analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}