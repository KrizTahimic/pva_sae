% !TEX root = main.tex
\chapter{Introduction}
\label{sec:intro}    %--note: labels help you with hyperlink editing (using your IDE)

\section{Background of the Study}
\label{sec:backgroundofthestudy}

The integration of Large Language Models (LLMs) into software development has sparked a fundamental shift in how code is written and maintained across the industry. A comprehensive study by \citeA{dohmke2023sea} revealed out of nearly one million developers, approximately 30\% of all AI-suggested code from GitHub Copilot is accepted and integrated into production systems. The same paper also projected that these productivity enhancements could contribute significantly to economic growth, potentially boosting global GDP by over USD 1.5 trillion by 2030 as these technologies help meet the growing worldwide demand for software development.

However, the widespread adoption of code-capable LLMs brings critical concerns regarding their reliability and correctness. Research has consistently documented troubling patterns in LLM-generated code quality. A comprehensive empirical study by \cite{tambon2025bugs} analyzed 333 bugs from leading code generation models and identified 10 distinctive bug patterns that persist across different LLMs. The propensity for errors is particularly pronounced: recent studies show that in bug-prone contexts, LLMs generate buggy code nearly as often as correct code, with even advanced models like GPT-4 achieving only 12.27\% accuracy when completing bug-prone code \cite{guo2025llms}. Perhaps most alarmingly, 44\% of the bugs LLMs produce are completely identical to historical bugs from their training data, with GPT-4o reproducing known errors 82.61\% of the time \cite{guo2025llms}. These correctness issues become critical in high-risk domains such as healthcare, banking, and military applications, where the consequences of defective code can be severe.

Despite the widespread adoption and associated risks, the internal mechanisms of these models remain poorly understood, operating largely as black boxes. This challenge has led researchers to explore various interpretability methods to understand and improve these models. Among these approaches, Mechanistic Interpretability has emerged as a promising direction, focusing on revealing how models represent and process information internally. This approach is especially valuable because it helps identify why models make certain mistakes or errors. The insights gained from mechanistic interpretability can inform better architectural choices for neural networks, making it particularly impactful for safety-critical applications.

In their pursuit to mechanistically understand neural networks, researchers discovered a critical challenge: superposition. This phenomenon reveals that neural networks can encode more features than they have available neurons, which might have been the cause of polysemantic neurons. To untangle these overlapping features, researchers turned to Sparse Autoencoders (SAE), which have proven effective at separating these intertwined features into more interpretable components.

After deciding to use SAE to disentangle polysemantic features, researchers face the crucial task of labeling and understanding each latent direction. Some researchers, like \citeA{bricken2023monosemanticity} and \citeA{templeton2024scaling}, employ a top-down approach that uses a larger LLM to generate labels for each latent direction based on tokens they activate strongly. In contrast, this paper follows what \citeA{ferrando2024know} implements, which is a more bottom-up approach that starts with a label or behavior of interest. Their methodology begins with constructing a dataset to elicit a specific latent direction, which will be inputted into the model. The model's internals will then be observed to identify what latent directions activate in response to the feature-specific dataset. 

\citeA{ferrando2024know} study uncovered specific latent directions in the model's representation space that encode the model's ability to recognize entities. Using datasets spanning various entity types, including people, movies, cities, and songs, the researchers identified distinct latent directions - some that activate specifically for entities the model can recall facts about and others that activate for entities the model doesn't recognize. Through model steering of these entity recognition latent directions, they demonstrated causal effects on model behavior, showing they could influence whether models would refuse to answer questions about known entities or generate hallucinated information about unknown ones. Their analysis revealed these entity recognition latent directions emerged in the middle layers of the network and regulated how attention mechanisms extracted factual information, providing crucial insights into how models determine whether to generate information or refuse to answer.

While these approaches have successfully revealed latent directions in natural language generation, the application of Sparse Autoencoders to understand code correctness latent directions in code-capable LLMs remains unexplored. This represents a significant opportunity to advance our understanding of how models internally represent code correctness and what \citeA{ferrando2024know} term self-knowledge, which they define as internal representations about a model's own capabilities. Specifically, we investigate whether models encode representations about their ability to generate correct code, a critical aspect for improving the reliability and safety of AI-assisted software development.

%%
%% --- 1.2 Research Objectives --- %%
%%

\section{Research Objectives}

To mechanistically interpret how Large Language Models (LLMs) encode and utilize code correctness mechanisms in their internal representations.

The specific objectives of this research are:
\begin{enumerate}
\item To identify four distinct types of code correctness latent directions in LLMs through Sparse Autoencoder analysis: correct-predicting and incorrect-predicting directions (using t-statistics), and correct-steering and incorrect-steering directions (using separation scores)
\item To evaluate the predictive capability of prediction directions through quantitative analysis, including AUROC, F1 scores, complexity robustness testing, and temperature robustness testing
\item To establish causal sufficiency of steering directions through activation steering interventions on generated code correctness
\item To establish causal necessity of correct directions through weight orthogonalization experiments
\item To investigate how steering interventions affect model attention mechanisms across prompt components (problem descriptions, test cases, and code initiators)
\item To determine whether code correctness mechanisms persist from base to instruction-tuned models
\end{enumerate}

%%
%% --- 1.3 Scope and Limitations --- %%
%%

\section{Scope and Limitations of the Research}
\label{sec:scopelimitations}

% Introduction
This research examines self-knowledge mechanisms in language models, explicitly focusing on code correctness. It is crucial to emphasize that our investigation of this self-knowledge does not suggest the presence of consciousness or other forms of subjective experience.

To investigate code correctness mechanisms, we employ the MBPP (Mostly Basic Python Problems) dataset as prompts, which consists of 1,000 Python programming problems designed to evaluate fundamental programming skills. While our decision to use a Python-specific dataset limits our ability to make claims about language-agnostic latent directions, it allows for a more manageable and focused investigation.

The selection of MBPP dataset \cite{austin2021program} over alternatives such as APPS \cite{austin2021program},  HumanEval \cite{chen2021evaluating}, and SWE-Bench \cite{jimenez2023swe} was guided by the need to ensure \texttt{gemma-2-2b} could generate both correct and incorrect solutions, enabling us to extract meaningful latent directions for both cases. While more powerful models could enable analysis of more challenging datasets like APPS, HumanEval, or SWE-Bench, our current model selection necessitated using MBPP's more accessible problems. This methodological choice constrains our ability to generalize findings to more complex, authentic programming scenarios as found in datasets like SWE-Bench \cite{jimenez2023swe}. Despite these limitations, MBPP provides a suitable foundation for our initial analysis, though future work with larger, more diverse datasets would strengthen and potentially expand our findings.

% Model Implementation and Code Generation
In our implementation, we utilize Google's Gemma 2 language model \cite{team2024gemma}, specifically employing its base configuration with 2 billion parameters, strategically minimizing computational requirements for LLM generation. Moreover, a key advantage of choosing Gemma 2 is the availability of Gemma Scope \cite{lieberum2024gemma}, an already trained Sparse Autoencoder suite designed explicitly for Gemma 2 models. Gemma Scope significantly reduces the computational burden of training our SAE by providing ready-to-use disentangled feature representations. While pre-trained SAEs significantly reduce computational requirements for our analysis, our findings remain limited by these models' reconstruction accuracy and the specific latent direction features they have learned. Our primary analysis focuses on the Gemma 2 Base model, for which GemmaScope was explicitly trained. Following \citeA{ferrando2024know}'s investigation of latent direction repurposing between base and instruct variants, we extend our analysis to test whether selected code correctness directions persist in the instruction-tuned variant, examining mechanism stability across fine-tuning.

% Logit Lens Subjectivity
Our interpretability analysis incorporates logit lens analysis to examine which tokens SAE features promote or suppress, providing qualitative insights into feature semantics. However, this interpretative technique carries inherent subjectivity: determining whether a feature encodes 'code correctness patterns' versus other semantic concepts requires human judgment based on observed token preferences. Different researchers may interpret the same logit lens visualizations differently, particularly when features show mixed or ambiguous token preferences. We mitigate this limitation by grounding our interpretations in quantitative validation through AUROC scores, F1 metrics, and causal interventions, using logit lens as supporting rather than primary evidence for feature identification.

% Binary classification.
Our code assessment methodology implements a binary classification system for generated code. Solutions are categorized as 'correct' if they successfully pass@1 all MBPP test cases for a given problem. Any solution that fails to pass these test cases, whether due to compilation errors, runtime exceptions, or incorrect outputs, is classified as 'incorrect.' Although this binary classification system provides a clear quantitative metric, future research could explore more nuanced evaluation methods, such as error type categorization and code complexity analysis. Furthermore, the dependence on MBPP's test cases introduces inherent limitations regarding the extent of test coverage and robust edge case detection.

We selected residual streams based on two key advantages: enhanced computational efficiency due to their smaller dimensionality compared to MLP layers, and the ability to partially address cross-layer superposition since residual streams aggregate outputs from all previous layers, providing a more holistic view of the model's internal representations \cite{templeton2024scaling}. However, this methodological decision carries an inherent limitation: code correctness latent directions may potentially be encoded in other architectural components such as attention mechanisms or MLP layers rather than residual streams. 

% Token Position Selection
When determining which token position to analyze, we considered that \citeA{ferrando2024know} examined multiple strategic positions depending on the phenomenon studied: final entity tokens when identifying entity recognition latents in the base model, and additionally final prompt tokens when investigating refusal behavior in the chat model. Their multi-position approach demonstrates that different computational mechanisms may be most salient at different representational locations. However, since programming problems lack clear entity subjects comparable to those in Ferrando et al.'s factual recall tasks, we follow \citeA{marks2023geometry}'s approach of analyzing representations at the final prompt token, where they found truth-related information tends to accumulate for factual statements. This position is particularly relevant for our study as it captures the model's culminating understanding after processing the entire programming problem description and test cases, analogous to how final prompt tokens in \citeA{ferrando2024know} captured knowledge about whether to refuse answering. We hypothesize this is the point where code correctness would be most strongly encoded, as the model has aggregated all problem constraints before beginning solution generation. Nevertheless, this methodological decision has limitations, as it may miss critical latent directions activated by earlier tokens (e.g., during problem comprehension) or intermediate states during code solution generation, suggesting future work should explore activation patterns across multiple token positions as demonstrated effective in prior interpretability research.

% Causal Validation and Model Steering Limitations
Our causal validation employs activation steering, weight orthogonalization, and attention analysis to establish both sufficiency and necessity of identified directions. However, the methodology focuses on steering individual latent activations in isolation, which may oversimplify the complex interplay between different latent directions in the model's representation space. This isolated approach cannot fully capture potential emergent properties that might arise from the activation combination of multiple code correctness latent directions, potentially limiting our understanding of how these latent directions work together in practice.

We extend our analysis to instruction-tuned models to test whether code correctness mechanisms persist after fine-tuning. However, this investigation remains limited to the Gemma 2 model family and may not generalize to other architectures or training approaches. Similarly, while we test predictor robustness across temperature variations, this represents only a subset of possible generation configurations.

% Bias Considerations / ethical consideration
Our study acknowledges specific research bias considerations relevant to code correctness analysis. When analyzing statistical significance variations across different temperatures and problem complexities, conclusions about why certain conditions perform differently may reflect researcher preconceptions. We mitigate these potential biases through our data-driven approach and statistical validation techniques throughout our experimental workflow.

% Future Direction
Future research initiatives could address these methodological limitations through several approaches: expanding the analysis to encompass multiple programming languages, incorporating a broader spectrum of programming tasks, examining larger and different architecture of language models.


%%
%% --- 1.4 Significance of the Research --- %%
%%

\section{Significance of the Research}

The significance of this research extends across theoretical understanding, practical applications, and safety implications for AI-assisted software development.

From a practical perspective, understanding the mechanisms of code correctness directly addresses a critical challenge in code-generating LLMs: ensuring reliable and correct code generation. This research investigates whether LLMs possess distinct mechanisms for predicting errors versus predicting correctness, and whether these mechanisms demonstrate different characteristics in terms of reliability and predictive power. Understanding these prediction mechanisms could provide critical insights into how models internally assess code quality, with significant implications for how we deploy and monitor AI-generated code in production environments. Furthermore, steering interventions on identified code correctness directions may enable targeted corrections to erroneous code. However, such interventions likely involve substantial tradeoffs between fixing errors and preserving initially correct code, necessitating careful consideration of when and how to apply these techniques.

The mechanistic insights from this research could translate into three practical applications for improving code generation systems. First, attention analysis may reveal which prompt components drive successful code generation, potentially demonstrating whether test cases or problem descriptions contribute more significantly to model understanding. Such findings could inform prompting strategies that optimize how programming problems are presented to models. Second, if reliable error prediction mechanisms can be identified, they could serve as automated alarm systems, flagging potentially incorrect generations for developer review before deployment. Third, these same predictor directions could potentially guide selective steering approaches, enabling interventions only when errors are anticipated, thereby avoiding the corruption that universal steering might introduce. Together, these potential applications demonstrate how mechanistic understanding could enable practical improvements in AI-assisted software development.

Economically, with approximately 30\% of all AI-suggested code being accepted into production systems \cite{dohmke2023sea}, enhancing the reliability of these suggestions directly affects the quality of deployed software across industries. This widespread adoption demonstrates significant potential, with the same research projecting that generative AI developer productivity benefits could boost global GDP by over USD 1.5 trillion by 2030 by helping meet growing software development demands. Furthermore, according to \citeA{krasner2022cost}, poor software quality costs the US economy approximately \$2.41 trillion in 2022 alone. Even modest improvements in code correctness prediction could translate to substantial saved costs.

From a safety perspective, this research addresses the critical need for reliable code generation in high-risk domains. As LLMs are increasingly deployed for code generation in healthcare systems, financial applications, and military infrastructure, the consequences of incorrect code extend beyond simple bugs to potential security vulnerabilities, data breaches, or system failures. By developing techniques to identify and control code correctness mechanisms, this research contributes to the broader goal of creating trustworthy AI systems.

Methodologically, this research extends mechanistic interpretability techniques, particularly SAE, to the domain of programming languages, bridging a significant gap in our understanding of how LLMs process and generate code. By examining the model's internal representations rather than just input-output behavior, this work provides unique insights into the mechanisms driving code validation within these systems. The application of SAEs to identify code correctness latent directions establishes a novel framework that could be applied to analyze other aspects of code generation, such as security awareness or algorithmic complexity understanding. Additionally, this research contributes a curated dataset of MBPP problems paired with \texttt{gemma-2-2b} generated solutions, each validated for correctness against test cases, offering a valuable dataset for future code generation research. Also, by adapting established validation approaches from \citeA{ferrando2024know} to the programming domain, this research demonstrates how these techniques can be effectively transferred to understand code generation mechanisms in LLMs beyond natural language generation.

The insights gained through this research may contribute to developing more transparent, reliable AI coding systems, aligning with growing regulatory and industry demands for AI interpretability. By illuminating the mechanisms of code correctness, this research lays essential groundwork for next-generation code generation models with enhanced capability to recognize the boundaries of their own knowledge.
