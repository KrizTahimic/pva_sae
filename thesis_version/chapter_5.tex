% !TEX root = main.tex
\chapter{Conclusions and Recommendations}
\label{sec:conclusion}

\section{Conclusions}

\textbf{Objective Completion.} This study successfully met all six research objectives. We identified four distinct types of code correctness latent directions (Objective 1) through dual selection metrics: t-statistics for predictor directions (correct-predicting and incorrect-predicting) and separation scores for steering directions (correct-steering and incorrect-steering). We validated predictor directions through AUROC, F1 scores, complexity robustness, and temperature robustness testing (Objective 2), with incorrect-predicting directions achieving F1=0.821 and maintaining stability across temperature variations (0.0-1.4), while correct-predicting directions achieved F1=0.504 with formatting-based degradation. We established causal sufficiency through activation steering interventions (Objective 3), demonstrating correct-steering produces 4.04\% correction ($p<0.001$) and 14.66\% corruption, while incorrect-steering achieves 64.7\% corruption. We confirmed causal necessity through weight orthogonalization (Objective 4), where correct direction removal caused 83.6\% corruption versus 19.0\% for controls ($p<0.001$), demonstrating these features are necessary for executable code generation. We investigated attention mechanism effects (Objective 5), revealing a 27.29 percentage point differential in test case attention (correct-steering +14.60, incorrect-steering -12.69) while problem descriptions showed minimal change. Finally, we validated mechanism persistence across model variants (Objective 6), with predictors maintaining F1$>$0.75 (base: 0.821, instruction-tuned: 0.772) and steering maintaining statistical significance (base: 4.04\%, instruction-tuned: 2.93\%, both $p<0.001$) despite instruction-tuning improving baseline performance from 29.9\% to 38.4\%.

\textbf{Technical Synthesis.} The experiments revealed complementary asymmetries: t-statistics effectively identified incorrect-predicting features (F1=0.821) while failing for correct-predicting features (F1=0.504), whereas separation scores effectively identified correct-steering features (necessary for generation) while failing for incorrect-steering features. This reflects fundamentally different mechanisms: prediction through graded confidence signals, steering through switch-like categorical activations. Steering interventions improved buggy code (4.04\%) but corrupted correct code (14.66\%), suggesting selective rather than universal intervention. Attention analysis revealed test case processing drives generation more than problem understanding (27.29 percentage point differential). These mechanisms persist across model variants, suggesting pre-training representations are repurposed during fine-tuning.

\textbf{Practical Implications.} The findings enable three applications: incorrect-predicting directions as error alarms (F1=0.821), prompting strategies emphasizing test examples over problem descriptions (27.29 percentage point attention differential), and selective steering combining predictors with intervention to avoid corrupting already-correct code.

\textbf{Novel Contribution.} To our knowledge, this represents the first application of Sparse Autoencoders to mechanistically interpret code correctness in Large Language Models. By adapting interpretability techniques from entity recognition studies \cite{ferrando2024know} to programming tasks, this research bridges a critical gap between mechanistic interpretability methods developed for natural language and their application to code generation systems, establishing a framework for understanding how models internally represent their ability to distinguish correct from incorrect code.

\section{Recommendations}

\subsection{Practical Applications}

Our findings enable three immediate applications for improving AI-assisted software development:

\textbf{Prompting Strategies for Enhanced Generation.} Developers and organizations deploying code-generating LLMs should prioritize test case examples in prompts over problem descriptions. Our attention analysis revealed a 27.29 percentage point differential, demonstrating that test case processing drives successful generation more than problem understanding. Prompt engineering frameworks should structure programming requests to emphasize concrete input-output examples rather than abstract problem statements.

\textbf{Error Detection in Development Environments.} Incorrect-predicting directions can be integrated into IDEs and code review tools as automated error alarm systems. With F1=0.821 performance and stability across temperature variations (0.0-1.4), these predictors can flag potentially incorrect AI-generated code for developer review before deployment. Implementation could take the form of IDE plugins, API services, or CI/CD pipeline checks that assess code correctness likelihood before merging.

\textbf{Selective Steering for Code Correction.} Rather than universal application, steering interventions should be triggered only when predictor directions indicate high error likelihood. This selective approach avoids the 14.66\% corruption rate of universal intervention while capturing the 4.04\% correction benefit for genuinely buggy code. Systems could combine t-statistic predictors with separation-score steering to apply intelligent, context-aware corrections.

\subsection{Future Research Directions}

Several methodological constraints create opportunities for extending this work:

\textbf{Cross-Model Generalization.} Our analysis focused exclusively on Gemma 2 2B \cite{team2024gemma}, chosen for computational efficiency and availability of pre-trained GemmaScope SAEs \cite{lieberum2024gemma}. Future research should systematically investigate code correctness directions across diverse model families (GPT, LLaMA, Mistral, Qwen) and parameter scales (1B-70B+). This comparative analysis would reveal whether our findings represent universal patterns in code processing or architecture-specific phenomena, and how latent directions transform across model sizes to inform efficient model design.

\textbf{Language and Dataset Extension.} The Python-specific MBPP dataset enabled focused analysis but constrains claims about language-agnostic correctness mechanisms. Extending analysis to multilingual benchmarks \cite{cassano2023multiple, li2022competition} would test whether code correctness directions transfer across languages or manifest through language-specific representations. Additionally, applying techniques to production-grade scenarios using SWE-Bench Verified \cite{chowdhury2024swebench} would validate whether mechanisms identified for basic problems scale to real-world development involving multiple files, dependencies, and sophisticated debugging.

\textbf{Alternative Interpretability Methods.} Beyond SAEs, alternative interpretability methods like cross-coders \cite{lindsey2024crosscoders} and transcoders \cite{dunefsky2024transcoders} remain unexplored for code correctness mechanisms. Comparing these approaches would determine which techniques most effectively identify correctness mechanisms and establish methodological best practices.

\textbf{Architectural Component Analysis.} Our analysis focused on residual streams due to computational efficiency and their ability to aggregate information across layers \cite{templeton2024scaling}. However, code correctness directions may exist in other architectural components such as MLP layers, attention mechanisms, attention output projections, or individual attention heads. Future work should explore these components to determine whether correctness mechanisms are concentrated in residual streams or distributed across the architecture.

\textbf{Circuit Analysis.} We identified code correctness directions but did not investigate what upstream features activate them. Circuit analysis could trace which prior features (syntax, semantics, test case matching) trigger code correctness latents, revealing the computational dependencies and information flow that enable correctness prediction in LLMs.

\textbf{Expanded Mechanistic Investigations.} Beyond code correctness, the framework established in this research could investigate other critical aspects of code generation, such as security vulnerability awareness, algorithmic efficiency understanding, or edge case handling. Identifying latent directions for these properties could further enhance the safety and utility of code-generating LLMs.